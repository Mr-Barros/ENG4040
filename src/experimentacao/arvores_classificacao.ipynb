{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'Mr-Barros'\n",
    "df = pd.read_csv('../../dados/base/chess_games_chesscom.csv')\n",
    "df = df[df['player'] == username]\n",
    "\n",
    "print(f'{username} games: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_base(data: pd.DataFrame):\n",
    "    df = data.copy()\n",
    "    # We only want to analyse normal chess games\n",
    "    df = df[df['rules'] == 'chess']\n",
    "    \n",
    "    # Add the 'result' column\n",
    "    def determine_result(row):\n",
    "        if row['winner'] == 'draw':\n",
    "            return 'draw'\n",
    "        elif row['winner'] == row['player_pieces']:\n",
    "            return 'win'\n",
    "        else:\n",
    "            return 'loss'\n",
    "    \n",
    "    df.loc[:, 'result'] = df.apply(determine_result, axis=1)\n",
    "\n",
    "    # makes it so the value is in reference to the player advantage\n",
    "    df.loc[df['player_pieces'] == 'black', 'opening_eval'] = (-1)*df.loc[df['player_pieces'] == 'black', 'opening_eval']\n",
    "    df.loc[df['player_pieces'] == 'black', 'midgame_eval'] = (-1)*df.loc[df['player_pieces'] == 'black', 'midgame_eval']\n",
    "\n",
    "    def treat_time_control(row):\n",
    "        time_control = row['time_control']\n",
    "        if '+' in row['time_control']:\n",
    "            time, increment = time_control.split('+')\n",
    "        elif '/' in row['time_control']:\n",
    "            time = time_control.split('/')[1]\n",
    "            if int(time) == 0:\n",
    "                time = 9999999 # no time limit\n",
    "            increment = 0\n",
    "        else:\n",
    "            time = row['time_control']\n",
    "            increment = 0\n",
    "        return pd.Series([int(time), int(increment)], index=['time_control', 'increment'])\n",
    "\n",
    "    df.loc[:, ['time_control', 'increment']] = df.apply(treat_time_control, axis=1)\n",
    "\n",
    "    df = df.drop([\n",
    "        df.columns[0], \n",
    "        'url',\n",
    "        'pgn', \n",
    "        'rated',\n",
    "        'accuracies',\n",
    "        'end_time',\n",
    "        'rules', \n",
    "        'tcn', \n",
    "        'winner',\n",
    "        'player',\n",
    "        'opponent',\n",
    "        'uuid', \n",
    "        'initial_setup', \n",
    "        'fen', \n",
    "        'start_time', \n",
    "        'move_list',\n",
    "        'move_evals',\n",
    "        'material_count'\n",
    "        ], axis=1)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "df = clean_base(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\\n\")\n",
    "print(f\"Columns: {df.columns}\\n\")\n",
    "for column in ['time_control', 'increment', 'time_class', 'player_pieces', 'win_method', 'opening_eval', 'midgame_eval']:\n",
    "    print(f'Unique values of {column}: {df[column].unique()}')\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_control'].value_counts()\n",
    "df['time_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oe_time_class = OrdinalEncoder(categories=[['bullet', 'blitz', 'rapid', 'daily']])\n",
    "ohe_eco = OneHotEncoder(sparse_output=False).set_output(transform='pandas')\n",
    "le_player_pieces = LabelEncoder()\n",
    "ohe_win_method = OneHotEncoder(sparse_output=False).set_output(transform='pandas')\n",
    "oe_result = OrdinalEncoder(categories=[['loss', 'draw', 'win']])\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df['time_class'] = oe_time_class.fit_transform(df[['time_class']])\n",
    "df['player_pieces'] = le_player_pieces.fit_transform(df['player_pieces'])\n",
    "# df['result'] = oe_result.fit_transform(df[['result']])\n",
    "\n",
    "eco_encoded = ohe_eco.fit_transform(df[['eco']])\n",
    "win_method_encoded = ohe_win_method.fit_transform(df[['win_method']]) \n",
    "\n",
    "df = pd.concat([df, eco_encoded, win_method_encoded], axis=1)\n",
    "df.drop(columns=['eco', 'win_method'], axis = 1, inplace = True)\n",
    "\n",
    "df[df.columns.drop('result')] = scaler.fit_transform(df[df.columns.drop('result')])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(data: pd.DataFrame, columns_to_remove: list[str]) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    for column in df.columns:\n",
    "        if any(column_to_remove in column for column_to_remove in columns_to_remove):\n",
    "            df.drop([column], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_outliers(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    for col in ['time_control', 'opponent_rating', 'winrate_with_opening']:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # for winrate_with_opening, substitute outliers by mean value\n",
    "        if col == 'winrate_with_opening':\n",
    "            col_mean = df[col].mean()\n",
    "            out_of_bounds = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
    "            df.loc[out_of_bounds, col] = col_mean           \n",
    "        else:\n",
    "            df = df[(df[col] > lower_bound) & (df[col] < upper_bound)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "test = treat_outliers(df)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will try combinations removing the following columns: \n",
    "# time_control, increment, win_method, winrate_with_opening\n",
    "column_combinations = [\n",
    "    ['win_method', 'winrate_with_opening'],\n",
    "    ['time_control', 'increment', 'win_method', 'winrate_with_opening'],\n",
    "    ['time_control', 'increment'],\n",
    "    []\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 7, 10],\n",
    "            'min_samples_leaf': [3, 10, 25],\n",
    "        }\n",
    "\n",
    "model_params = {\n",
    "    'Árvore de Classificação': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'splitter': ['best', 'random'],\n",
    "            'max_depth': [None, 7, 10],\n",
    "            'min_samples_leaf': [3, 10, 25],\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_tuples([\n",
    "    ('Pré-Processamento', 'Outliers', 'manter'),\n",
    "    ('Pré-Processamento', 'Outliers', 'remover'),\n",
    "\n",
    "    ('Pré-Processamento', 'Remover Coluna', 'time_control'),\n",
    "    ('Pré-Processamento', 'Remover Coluna', 'increment'),\n",
    "    ('Pré-Processamento', 'Remover Coluna', 'win_method'),\n",
    "    ('Pré-Processamento', 'Remover Coluna', 'winrate_with_opening'),\n",
    "\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'criterion={params[\"criterion\"][0]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'criterion={params[\"criterion\"][1]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'criterion={params[\"criterion\"][2]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'splitter={params[\"splitter\"][0]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'splitter={params[\"splitter\"][1]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'max_depth={params[\"max_depth\"][0]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'max_depth={params[\"max_depth\"][1]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'max_depth={params[\"max_depth\"][2]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'min_samples_leaf={params[\"min_samples_leaf\"][0]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'min_samples_leaf={params[\"min_samples_leaf\"][1]}'),\n",
    "    ('Mineração de Dados', 'Árvore de Classificação', f'min_samples_leaf={params[\"min_samples_leaf\"][2]}'),\n",
    "\n",
    "    ('Pós-Processamento', 'Medidas', 'Acurácia'),\n",
    "    ('Pós-Processamento', 'Medidas', 'Precisão'),\n",
    "    ('Pós-Processamento', 'Medidas', 'Recall'),\n",
    "    ('Pós-Processamento', 'Medidas', 'F-Measure')\n",
    "])\n",
    "\n",
    "experiment = pd.DataFrame(columns=columns)\n",
    "experiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorers = {\n",
    "    'Accuracy': make_scorer(accuracy_score),\n",
    "    'Precision': make_scorer(precision_score, average='weighted'),\n",
    "    'Recall': make_scorer(recall_score, average='weighted'),\n",
    "    'F1-Score': make_scorer(f1_score, average='weighted')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: figure out how to handle multiple class classification (the precision metric is doing a division by 0)\n",
    "df = df[df['result'] != 'draw']\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(df, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for remove_outliers in [True, False]:\n",
    "    for combination in column_combinations:\n",
    "        train_treated = remove_columns(train, combination)\n",
    "        if remove_outliers:\n",
    "            train_treated = treat_outliers(train)\n",
    "\n",
    "        \n",
    "\n",
    "        X_train = train_treated.drop('result', axis=1)\n",
    "        y_train = train_treated['result']\n",
    "\n",
    "        \n",
    "        for model_name, mp in model_params.items():\n",
    "            clf = GridSearchCV(mp['model'], mp['params'], cv=5, scoring=scorers, return_train_score=False, refit='F1-Score')\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            results = pd.DataFrame(clf.cv_results_)[['params', 'mean_test_Accuracy', 'mean_test_Precision', 'mean_test_Recall', 'mean_test_F1-Score']]\n",
    "\n",
    "            for _, metrics in results.iterrows():\n",
    "                row = pd.Series(index=columns, dtype=str)\n",
    "                row[:] = ' '\n",
    "\n",
    "                if remove_outliers:\n",
    "                    row[('Pré-Processamento', 'Outliers', 'remover')] = 'x'\n",
    "                else:\n",
    "                    row[('Pré-Processamento', 'Outliers', 'manter')] = 'x'\n",
    "                \n",
    "                for column in combination:\n",
    "                    row[('Pré-Processamento', 'Remover Coluna', column)] = 'x'\n",
    "                \n",
    "                for param_name, param_value in metrics['params'].items():\n",
    "                    row[('Mineração de Dados', f'{model_name}', f'{param_name}={param_value}')] = 'x'\n",
    "                \n",
    "                row[('Pós-Processamento', 'Medidas', 'Acurácia')] = metrics['mean_test_Accuracy']\n",
    "                row[('Pós-Processamento', 'Medidas', 'Precisão')] = metrics['mean_test_Precision']\n",
    "                row[('Pós-Processamento', 'Medidas', 'Recall')] = metrics['mean_test_Recall']\n",
    "                row[('Pós-Processamento', 'Medidas', 'F-Measure')] = metrics['mean_test_F1-Score']\n",
    "\n",
    "                experiment.loc[len(experiment)] = row \n",
    "\n",
    "print(experiment.shape)\n",
    "experiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Excel\n",
    "experiment.to_excel('decision_tree_grid_search_results.xlsx')\n",
    "print(\"Results saved to 'decision_tree_grid_search_results.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=10, min_samples_leaf=10)\n",
    "columns_to_remove = []\n",
    "\n",
    "train_treated = remove_columns(train, columns_to_remove)\n",
    "train_treated = treat_outliers(train_treated)\n",
    "\n",
    "X_train = train_treated.drop('result', axis=1)\n",
    "y_train = train_treated['result']\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "test_treated = remove_columns(test, columns_to_remove)\n",
    "test_treated = treat_outliers(test_treated)\n",
    "\n",
    "X_test = train_treated.drop('result', axis=1)\n",
    "y_test = train_treated['result']\n",
    "\n",
    "pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "precision = precision_score(y_test, pred, pos_label='win')\n",
    "recall = recall_score(y_test, pred, pos_label='win')\n",
    "f1 = f1_score(y_test, pred, pos_label='win')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4%}\")\n",
    "print(f\"Precision: {precision:.4%}\")\n",
    "print(f\"Recall: {recall:.4%}\")\n",
    "print(f\"F1 Score: {f1:.4%}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
